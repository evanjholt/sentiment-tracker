{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP57KDAtnCwC7b1e/baOvhz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evanjholt/sentiment-tracker/blob/main/Reddit_Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47JAgPX1UsBP",
        "outputId": "c30ff0b9-f22a-440e-ed99-7344e0c2ae2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pandas as pd # Useful for data analysis and quick table views"
      ],
      "metadata": {
        "id": "fqWg1DpUVPYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load Stock Tickers from CSV (using GitHub Raw Link)\n",
        "\n",
        "# --- IMPORTANT: Replace this URL with the actual raw link you copied from GitHub ---\n",
        "csv_url = 'https://raw.githubusercontent.com/abbadata/stock-tickers/refs/heads/main/data/nasdaq.csv'\n",
        "\n",
        "# --- IMPORTANT: Verify the column name in your chosen CSV that contains the tickers ---\n",
        "ticker_column_name = 'Symbol' # Common for NASDAQ/NYSE lists. Could be 'Ticker', 'Stock', etc.\n",
        "\n",
        "try:\n",
        "    print(f\"Attempting to load tickers from: {csv_url}\")\n",
        "    all_us_tickers_df = pd.read_csv(csv_url)\n",
        "\n",
        "    # Basic data cleaning and validation for tickers\n",
        "    all_us_tickers_df = all_us_tickers_df.dropna(subset=[ticker_column_name])\n",
        "    all_us_tickers_df = all_us_tickers_df[all_us_tickers_df[ticker_column_name].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "    raw_stock_tickers = all_us_tickers_df[ticker_column_name].unique().tolist() # Store as raw_stock_tickers temporarily\n",
        "\n",
        "    # Further clean and normalize tickers (alphanumeric, uppercase, typical length)\n",
        "    stock_tickers_processed = [t for t in raw_stock_tickers if t.isalnum() and 1 <= len(t) <= 5] # Tickers are usually 1-5 chars\n",
        "    stock_tickers_processed = [t.upper() for t in stock_tickers_processed]\n",
        "\n",
        "\n",
        "    # --- NEW: More robust common word exclusion ---\n",
        "    # 1. Start with a more comprehensive list of English stop words\n",
        "    # You can typically get these from NLTK or use a hardcoded list\n",
        "    # For Colab, let's just define a fairly large list of very common short words.\n",
        "    common_english_words = [\n",
        "        \"A\", \"AN\", \"AND\", \"ARE\", \"AS\", \"AT\", \"BE\", \"BECAUSE\", \"BUT\", \"BY\", \"FOR\", \"FROM\", \"HAS\",\n",
        "        \"HE\", \"HER\", \"HIS\", \"HOW\", \"I\", \"IN\", \"IS\", \"IT\", \"ITS\", \"OF\", \"ON\", \"OR\", \"THAT\", \"THE\",\n",
        "        \"THIS\", \"TO\", \"WAS\", \"WHAT\", \"WHEN\", \"WHERE\", \"WHO\", \"WHY\", \"WILL\", \"WITH\", \"YOU\",\n",
        "        \"YOUR\", \"ME\", \"MY\", \"WE\", \"OUR\", \"YOURS\", \"HIM\", \"HIS\", \"HER\", \"HERS\", \"THEY\", \"THEM\",\n",
        "        \"THEIR\", \"MINE\", \"DO\", \"DOES\", \"DID\", \"NOT\", \"NO\", \"YES\", \"AM\", \"ISN\", \"AREN\", \"WASN\",\n",
        "        \"WEREN\", \"HAVEN\", \"HASN\", \"HAD\", \"ABOUT\", \"ABOVE\", \"AFTER\", \"AGAIN\", \"ALL\", \"AMONG\",\n",
        "        \"ANY\", \"BELOW\", \"BETWEEN\", \"BOTH\", \"EACH\", \"FEW\", \"MORE\", \"MOST\", \"OTHER\", \"SOME\",\n",
        "        \"SUCH\", \"THAN\", \"TOO\", \"VERY\", \"S\", \"T\", \"CAN\", \"JUST\", \"DON\", \"SHOULD\", \"NOW\",\n",
        "        \"ONLY\", \"EVEN\", \"MUCH\", \"MANY\", \"ONE\", \"TWO\", \"THREE\", \"FOUR\", \"FIVE\", \"SIX\", \"SEVEN\",\n",
        "        \"EIGHT\", \"NINE\", \"TEN\", # Numbers as words\n",
        "        # Add problematic tickers you've already observed that are common words\n",
        "        'GOOD', 'HIGH', 'LOW', 'BIG', 'OLD', 'NEW', 'RUN', 'BUY', 'SELL', 'HOLD', 'READ', 'OUT',\n",
        "        'SEE', 'FUN', 'HUB', 'MAP', 'PINE', 'RIDE', 'SAP', 'MOON', 'QT', 'ZAP', 'UPL', 'WOLF',\n",
        "        'JOY', 'KISS', 'LUCK', 'VIEW', 'YUM', 'DOG', 'EAT', 'FROG', 'CAT', 'PUMP', 'DUMP', 'SPY', 'QQQ', 'VIX',\n",
        "        'AI', 'META', 'GOOG', 'COIN', 'SHOP', 'SNOW', 'ROKU', 'PLTR', 'MRNA' # Tickers that are also highly common names/words\n",
        "    ]\n",
        "    # Combine with any specific short, common words you've observed as false positives\n",
        "    # that might not be in a standard stop word list\n",
        "    additional_problem_words = [\n",
        "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
        "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z' # Single letters\n",
        "    ]\n",
        "    all_excluded_words_set = set(common_english_words + additional_problem_words)\n",
        "    all_excluded_words_set = {word.upper() for word in all_excluded_words_set} # Ensure uppercase for consistent matching\n",
        "\n",
        "    # --- Filter out the excluded tickers ---\n",
        "    stock_tickers = [t for t in stock_tickers_processed if t not in all_excluded_words_set]\n",
        "\n",
        "    print(f\"Loaded {len(stock_tickers_processed)} raw tickers. Excluded {len(stock_tickers_processed) - len(stock_tickers)} common word/excluded tickers.\")\n",
        "    print(f\"Final list has {len(stock_tickers)} unique tickers.\")\n",
        "    if len(stock_tickers) > 0:\n",
        "        print(f\"First 10 filtered tickers: {stock_tickers[:10]}\")\n",
        "    else:\n",
        "        print(\"No tickers remain after filtering. Check your exclusion list or CSV.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not load tickers from GitHub URL. Error: {e}\")\n",
        "    print(f\"WARNING: Using a smaller, hardcoded list as a fallback for testing.\")\n",
        "    # Fallback list (ensure this fallback also accounts for exclusions)\n",
        "    stock_tickers = [t for t in ['GME', 'AMC', 'TSLA', 'AAPL', 'MSFT', 'UNH', 'NVDA', 'AMD', 'BB', 'CLOV', 'SOFI', 'RIVN', 'AMZN', 'NFLX'] if t not in all_excluded_words_set] # Filter fallback too!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjJ_AM5QdZMT",
        "outputId": "e40edeed-2b77-4a7a-e6f5-d070bd11a6a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load tickers from: https://raw.githubusercontent.com/abbadata/stock-tickers/refs/heads/main/data/nasdaq.csv\n",
            "Loaded 4056 raw tickers. Excluded 16 common word/excluded tickers.\n",
            "Final list has 4040 unique tickers.\n",
            "First 10 filtered tickers: ['VCVC', 'VCVCU', 'VCVCW', 'TXG', 'YI', 'YQ', 'TURN', 'ATNF', 'ATNFW', 'FLWS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=userdata.get('REDDIT_CLIENT_ID'),\n",
        "    client_secret=userdata.get('REDDIT_CLIENT_SECRET'),\n",
        "    user_agent=userdata.get('REDDIT_USER_AGENT')\n",
        ")"
      ],
      "metadata": {
        "id": "Q2KnqlGEVzdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time # Ensure 'time' module is imported if not already in Cell 2\n",
        "\n",
        "def find_stock_mentions(text):\n",
        "    \"\"\"\n",
        "    Identifies stock tickers within a given text based on the global stock_tickers list.\n",
        "    \"\"\"\n",
        "    mentions = set()\n",
        "    # The 'stock_tickers' list is populated from Cell 3 and should be globally accessible\n",
        "    for ticker in stock_tickers:\n",
        "        # Look for exact word matches or $ticker (case-insensitive)\n",
        "        # Using a word boundary '\\b' to match whole words and prevent partial matches (e.g., 'CAT' in 'CATCH')\n",
        "        # Using '\\$' to match the dollar sign prefix (e.g., '$AAPL')\n",
        "        if re.search(r'\\b' + re.escape(ticker) + r'\\b', text, re.IGNORECASE) or \\\n",
        "           re.search(r'\\$' + re.escape(ticker) + r'\\b', text, re.IGNORECASE):\n",
        "            mentions.add(ticker.upper())\n",
        "    return list(mentions)\n",
        "\n",
        "def scrape_reddit_mentions():\n",
        "    \"\"\"\n",
        "    Scrapes a limited number of new submissions and their comments\n",
        "    from specified subreddits for stock mentions.\n",
        "    Returns a dictionary where keys are stock tickers and values are lists of mention details.\n",
        "    \"\"\"\n",
        "    stock_data = {}\n",
        "    # You can start with one or two subreddits for faster testing, then add more.\n",
        "    subreddits_to_monitor = ['stocks', 'investing'] # Start with these, add 'wallstreetbets' later if desired\n",
        "\n",
        "    print(\"Starting Reddit scraping job...\")\n",
        "    print(f\"Monitoring {len(stock_tickers)} tickers across {len(subreddits_to_monitor)} subreddits.\")\n",
        "\n",
        "    # --- Configuration for Scraping Limit and Pauses ---\n",
        "    # Adjust 'scrape_limit_per_subreddit' based on how much data you want and your observed speed.\n",
        "    # A value of 20-50 per subreddit is a good balance for testing.\n",
        "    scrape_limit_per_subreddit = 20 # Number of 'new' submissions to fetch per subreddit\n",
        "\n",
        "    # Pause duration: higher values mean slower but safer scraping.\n",
        "    pause_between_submissions = 1 # seconds to pause after a few submissions\n",
        "    pause_between_subreddits = 5 # seconds to pause after processing an entire subreddit\n",
        "\n",
        "    for subreddit_name in subreddits_to_monitor:\n",
        "        try:\n",
        "            subreddit = reddit.subreddit(subreddit_name)\n",
        "            print(f\"\\n--- Scraping r/{subreddit_name} (fetching top {scrape_limit_per_subreddit} new posts)... ---\")\n",
        "\n",
        "            # Fetch new submissions\n",
        "            for i, submission in enumerate(subreddit.new(limit=scrape_limit_per_subreddit)):\n",
        "                # Introduce a small pause periodically to respect Reddit API limits\n",
        "                if i > 0 and i % 5 == 0: # Pause every 5 submissions\n",
        "                    print(f\"  Pausing {pause_between_submissions}s after {i} submissions in r/{subreddit_name}...\")\n",
        "                    time.sleep(pause_between_submissions)\n",
        "\n",
        "                submission_text = submission.title + \" \" + submission.selftext # Combine title and body\n",
        "                mentions_in_submission = find_stock_mentions(submission_text)\n",
        "\n",
        "                if mentions_in_submission:\n",
        "                    # print(f\"    Found mentions in submission '{submission.title[:50]}...'\") # Uncomment for detailed debug\n",
        "                    for mention in mentions_in_submission:\n",
        "                        if mention not in stock_data:\n",
        "                            stock_data[mention] = []\n",
        "                        stock_data[mention].append({\n",
        "                            'type': 'submission',\n",
        "                            'subreddit': subreddit_name,\n",
        "                            'id': submission.id,\n",
        "                            'title': submission.title,\n",
        "                            'url': submission.url,\n",
        "                            'score': submission.score,\n",
        "                            'created_utc': submission.created_utc, # Unix timestamp\n",
        "                            'timestamp': datetime.fromtimestamp(submission.created_utc).isoformat() # ISO 8601 string\n",
        "                        })\n",
        "\n",
        "                # Process comments for each submission\n",
        "                # replace_more(limit=0) fetches only directly available comments,\n",
        "                # avoiding extra API calls for \"More Comments\" links which is good for testing.\n",
        "                submission.comments.replace_more(limit=0)\n",
        "                for comment in submission.comments.list():\n",
        "                    mentions_in_comment = find_stock_mentions(comment.body)\n",
        "                    if mentions_in_comment:\n",
        "                        # print(f\"      Found mentions in comment '{comment.body[:50]}...'\") # Uncomment for detailed debug\n",
        "                        for mention in mentions_in_comment:\n",
        "                            if mention not in stock_data:\n",
        "                                stock_data[mention] = []\n",
        "                            stock_data[mention].append({\n",
        "                                'type': 'comment',\n",
        "                                'subreddit': subreddit_name,\n",
        "                                'id': comment.id,\n",
        "                                'submission_id': submission.id, # Link comment to its parent submission\n",
        "                                'comment_body': comment.body,\n",
        "                                'url': f\"https://reddit.com{comment.permalink}\",\n",
        "                                'score': comment.score,\n",
        "                                'created_utc': comment.created_utc,\n",
        "                                'timestamp': datetime.fromtimestamp(comment.created_utc).isoformat()\n",
        "                            })\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to scrape r/{subreddit_name}. Error: {e}\")\n",
        "\n",
        "        # Pause after finishing an entire subreddit, gives a larger break before the next subreddit\n",
        "        print(f\"  Finished r/{subreddit_name}. Pausing for {pause_between_subreddits}s before next subreddit...\")\n",
        "        time.sleep(pause_between_subreddits)\n",
        "\n",
        "    print(\"\\nReddit scraping job finished.\")\n",
        "    return stock_data\n",
        "\n",
        "# --- Execute the scraper function and store results ---\n",
        "test_mentions = scrape_reddit_mentions()\n",
        "\n",
        "print(\"\\n--- Scrape Result Summary ---\")\n",
        "if test_mentions:\n",
        "    total_mentions_found = sum(len(data) for data in test_mentions.values())\n",
        "    print(f\"Found {total_mentions_found} mentions for {len(test_mentions)} unique stocks.\")\n",
        "    # Show top 5 mentioned stocks by raw count\n",
        "    sorted_stocks = sorted(test_mentions.items(), key=lambda item: len(item[1]), reverse=True)\n",
        "    print(\"\\nTop 5 Most Mentioned Stocks (Raw Count):\")\n",
        "    for stock, data in sorted_stocks[:5]:\n",
        "        print(f\"  - {stock}: {len(data)} mentions\")\n",
        "else:\n",
        "    print(\"No stock mentions found in this scrape. Try increasing scrape_limit_per_subreddit or adding more active subreddits/tickers.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUnQoACsn7lA",
        "outputId": "0323876a-6482-4464-8869-c3cedafc93fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Reddit scraping job...\n",
            "Monitoring 4040 tickers across 2 subreddits.\n",
            "\n",
            "--- Scraping r/stocks (fetching top 20 new posts)... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pausing 1s after 5 submissions in r/stocks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pausing 1s after 10 submissions in r/stocks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pausing 1s after 15 submissions in r/stocks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Finished r/stocks. Pausing for 5s before next subreddit...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Scraping r/investing (fetching top 20 new posts)... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pausing 1s after 5 submissions in r/investing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pausing 1s after 10 submissions in r/investing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Pausing 1s after 15 submissions in r/investing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Finished r/investing. Pausing for 5s before next subreddit...\n",
            "\n",
            "Reddit scraping job finished.\n",
            "\n",
            "--- Scrape Result Summary ---\n",
            "Found 534 mentions for 98 unique stocks.\n",
            "\n",
            "Top 5 Most Mentioned Stocks (Raw Count):\n",
            "  - GO: 51 mentions\n",
            "  - GOLD: 23 mentions\n",
            "  - NEXT: 23 mentions\n",
            "  - REAL: 22 mentions\n",
            "  - FUND: 22 mentions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Analyze Scraped Data with Pandas\n",
        "# This cell transforms the scraped data into a DataFrame for easier analysis and ranking.\n",
        "\n",
        "# Flatten the dictionary of mentions into a list of dictionaries\n",
        "all_mentions_list = []\n",
        "for ticker, mentions in test_mentions.items():\n",
        "    for mention_detail in mentions:\n",
        "        mention_detail['ticker'] = ticker # Add the stock ticker to each mention record\n",
        "        all_mentions_list.append(mention_detail)\n",
        "\n",
        "if all_mentions_list:\n",
        "    # Create a Pandas DataFrame from the list of mentions\n",
        "    df = pd.DataFrame(all_mentions_list)\n",
        "\n",
        "    print(\"\\n--- DataFrame Head (First 5 rows) ---\")\n",
        "    print(df.head()) # Shows the first 5 rows of your data\n",
        "\n",
        "    print(\"\\n--- Basic DataFrame Info ---\")\n",
        "    df.info() # Provides a summary of the DataFrame structure and data types\n",
        "\n",
        "    print(\"\\n--- Stock Mention Counts (Overall Ranking) ---\")\n",
        "    # Count how many times each stock ticker was mentioned\n",
        "    # This is your primary ranking based on raw mention count\n",
        "    print(df['ticker'].value_counts().head(15)) # Show top 15 by count\n",
        "\n",
        "    print(\"\\n--- Mentions by Type (Submission vs. Comment) ---\")\n",
        "    print(df['type'].value_counts())\n",
        "\n",
        "    print(\"\\n--- Subreddit Mention Counts ---\")\n",
        "    print(df['subreddit'].value_counts())\n",
        "\n",
        "    print(\"\\n--- Top 10 Submissions by Score (Most Engaged Posts) ---\")\n",
        "    # Filter for submissions and sort by 'score' (upvotes)\n",
        "    submissions_df = df[df['type'] == 'submission'].sort_values(by='score', ascending=False)\n",
        "    if not submissions_df.empty:\n",
        "        # Display relevant columns for top submissions\n",
        "        print(submissions_df.head(10)[['ticker', 'title', 'score', 'url', 'timestamp']])\n",
        "    else:\n",
        "        print(\"No submission data found to show top posts.\")\n",
        "\n",
        "    print(\"\\n--- Top 10 Comments by Score (Most Engaged Comments) ---\")\n",
        "    # Filter for comments and sort by 'score'\n",
        "    comments_df = df[df['type'] == 'comment'].sort_values(by='score', ascending=False)\n",
        "    if not comments_df.empty:\n",
        "        # Display relevant columns for top comments\n",
        "        print(comments_df.head(10)[['ticker', 'comment_body', 'score', 'url', 'timestamp']])\n",
        "    else:\n",
        "        print(\"No comment data found to show top comments.\")\n",
        "\n",
        "    # --- Advanced Analysis (Optional - uncomment and explore) ---\n",
        "    # Convert timestamp column to datetime objects for time-based analysis\n",
        "    # df['datetime'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "    # print(\"\\n--- Mentions by Hour (Example) ---\")\n",
        "    # if 'datetime' in df.columns:\n",
        "    #     df['hour'] = df['datetime'].dt.hour\n",
        "    #     print(df.groupby('hour')['ticker'].value_counts().unstack(fill_value=0))\n",
        "\n",
        "    # print(\"\\n--- Mentions per Stock over Time (Example) ---\")\n",
        "    # if 'datetime' in df.columns:\n",
        "    #     # Group by date and ticker, then count\n",
        "    #     daily_mentions = df.groupby([df['datetime'].dt.date, 'ticker']).size().unstack(fill_value=0)\n",
        "    #     print(daily_mentions.tail()) # Show last few days\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo mentions were found in the DataFrame to analyze.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3OuwspdgjLG",
        "outputId": "061be6cd-5049-4921-f96e-1ac61de72617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- DataFrame Head (First 5 rows) ---\n",
            "         type subreddit       id                                     title  \\\n",
            "0  submission    stocks  1ks8nyj  ETOR valuation against HOOD, IBKR & BULL   \n",
            "1     comment    stocks  mti3maq                                       NaN   \n",
            "2  submission    stocks  1ks8nyj  ETOR valuation against HOOD, IBKR & BULL   \n",
            "3     comment    stocks  mtizi2z                                       NaN   \n",
            "4     comment    stocks  mthg5aw                                       NaN   \n",
            "\n",
            "                                                 url  score   created_utc  \\\n",
            "0  https://www.reddit.com/r/stocks/comments/1ks8n...      1  1.747860e+09   \n",
            "1  https://reddit.com/r/stocks/comments/1ks1mx9/w...      2  1.747847e+09   \n",
            "2  https://www.reddit.com/r/stocks/comments/1ks8n...      1  1.747860e+09   \n",
            "3  https://reddit.com/r/stocks/comments/1ks6bjo/u...     11  1.747856e+09   \n",
            "4  https://reddit.com/r/stocks/comments/1krzdkd/e...      3  1.747840e+09   \n",
            "\n",
            "             timestamp ticker submission_id  \\\n",
            "0  2025-05-21T20:46:40   IBKR           NaN   \n",
            "1  2025-05-21T16:55:03   IBKR       1ks1mx9   \n",
            "2  2025-05-21T20:46:40   GROW           NaN   \n",
            "3  2025-05-21T19:26:49   GROW       1ks6bjo   \n",
            "4  2025-05-21T15:02:44   GROW       1krzdkd   \n",
            "\n",
            "                                        comment_body  \n",
            "0                                                NaN  \n",
            "1  Fellow Dutchie here ðŸ‘‹\\n\\nI find Ben Felix, Pla...  \n",
            "2                                                NaN  \n",
            "3  The US is pigeonholed. Damned if they do, damn...  \n",
            "4  agreeing with others to avoid the stock market...  \n",
            "\n",
            "--- Basic DataFrame Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1154 entries, 0 to 1153\n",
            "Data columns (total 11 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   type           1154 non-null   object \n",
            " 1   subreddit      1154 non-null   object \n",
            " 2   id             1154 non-null   object \n",
            " 3   title          163 non-null    object \n",
            " 4   url            1154 non-null   object \n",
            " 5   score          1154 non-null   int64  \n",
            " 6   created_utc    1154 non-null   float64\n",
            " 7   timestamp      1154 non-null   object \n",
            " 8   ticker         1154 non-null   object \n",
            " 9   submission_id  991 non-null    object \n",
            " 10  comment_body   991 non-null    object \n",
            "dtypes: float64(1), int64(1), object(9)\n",
            "memory usage: 99.3+ KB\n",
            "\n",
            "--- Stock Mention Counts (Overall Ranking) ---\n",
            "ticker\n",
            "ON      214\n",
            "CAN     135\n",
            "HAS      84\n",
            "GOOD     63\n",
            "ANY      62\n",
            "GO       53\n",
            "VERY     34\n",
            "FUND     25\n",
            "GOLD     25\n",
            "NEXT     24\n",
            "REAL     19\n",
            "CASH     17\n",
            "STAY     16\n",
            "RUN      16\n",
            "COST     14\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Mentions by Type (Submission vs. Comment) ---\n",
            "type\n",
            "comment       991\n",
            "submission    163\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Subreddit Mention Counts ---\n",
            "subreddit\n",
            "investing    607\n",
            "stocks       547\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Top 10 Submissions by Score (Most Engaged Posts) ---\n",
            "     ticker                                              title  score  \\\n",
            "20      HAS  UnitedHealth urges shareholders to back CEOâ€™s ...    494   \n",
            "1012   SHIP  UnitedHealth urges shareholders to back CEOâ€™s ...    494   \n",
            "495     CAN  UnitedHealth urges shareholders to back CEOâ€™s ...    494   \n",
            "248      ON  Dow tumbles more than 600 points as Treasury y...    310   \n",
            "912    TECH  Dow tumbles more than 600 points as Treasury y...    310   \n",
            "947    LIVE  Dow tumbles more than 600 points as Treasury y...    310   \n",
            "944    FAST  Dow tumbles more than 600 points as Treasury y...    310   \n",
            "305      ON  NYC hardos and international students canâ€™t st...    118   \n",
            "521     CAN  NYC hardos and international students canâ€™t st...    118   \n",
            "188      VS  NYC hardos and international students canâ€™t st...    118   \n",
            "\n",
            "                                                    url            timestamp  \n",
            "20    https://www.reddit.com/r/stocks/comments/1ks25...  2025-05-21T16:25:59  \n",
            "1012  https://www.reddit.com/r/stocks/comments/1ks25...  2025-05-21T16:25:59  \n",
            "495   https://www.reddit.com/r/stocks/comments/1ks25...  2025-05-21T16:25:59  \n",
            "248   https://www.reddit.com/r/stocks/comments/1ks4o...  2025-05-21T18:06:52  \n",
            "912   https://www.reddit.com/r/stocks/comments/1ks4o...  2025-05-21T18:06:52  \n",
            "947   https://www.reddit.com/r/stocks/comments/1ks4o...  2025-05-21T18:06:52  \n",
            "944   https://www.reddit.com/r/stocks/comments/1ks4o...  2025-05-21T18:06:52  \n",
            "305   https://www.reddit.com/r/stocks/comments/1kryh...  2025-05-21T13:58:08  \n",
            "521   https://www.reddit.com/r/stocks/comments/1kryh...  2025-05-21T13:58:08  \n",
            "188   https://www.reddit.com/r/stocks/comments/1kryh...  2025-05-21T13:58:08  \n",
            "\n",
            "--- Top 10 Comments by Score (Most Engaged Comments) ---\n",
            "     ticker                                       comment_body  score  \\\n",
            "401      ON   Give them each $25k and move on with your life.Â     241   \n",
            "868    LIFE   Give them each $25k and move on with your life.Â     241   \n",
            "948    LIVE    Only 60M, sad. They will barely be able to live    208   \n",
            "371      ON                   Rates on US treasuries moved up.    144   \n",
            "1039   GAIN  Thereâ€™s no free lunch in the stock market. 4x ...    111   \n",
            "447    FREE  Thereâ€™s no free lunch in the stock market. 4x ...    111   \n",
            "785    NVDA  Thereâ€™s no free lunch in the stock market. 4x ...    111   \n",
            "21      HAS  CEO pay for what, getting the stock destroyed?...    105   \n",
            "71      HAS  I would divide it up.\\nLeaving it invested tog...    100   \n",
            "380      ON  Bond holders are following that insane BIG BEA...     95   \n",
            "\n",
            "                                                    url            timestamp  \n",
            "401   https://reddit.com/r/investing/comments/1ks0wc...  2025-05-21T15:38:18  \n",
            "868   https://reddit.com/r/investing/comments/1ks0wc...  2025-05-21T15:38:18  \n",
            "948   https://reddit.com/r/stocks/comments/1ks2568/u...  2025-05-21T16:34:20  \n",
            "371   https://reddit.com/r/investing/comments/1ks6cy...  2025-05-21T19:15:23  \n",
            "1039  https://reddit.com/r/stocks/comments/1ks1tnn/w...  2025-05-21T16:34:15  \n",
            "447   https://reddit.com/r/stocks/comments/1ks1tnn/w...  2025-05-21T16:34:15  \n",
            "785   https://reddit.com/r/stocks/comments/1ks1tnn/w...  2025-05-21T16:34:15  \n",
            "21    https://reddit.com/r/stocks/comments/1ks2568/u...  2025-05-21T16:40:26  \n",
            "71    https://reddit.com/r/investing/comments/1ks0wc...  2025-05-21T15:40:43  \n",
            "380   https://reddit.com/r/investing/comments/1ks6cy...  2025-05-21T19:24:22  \n"
          ]
        }
      ]
    }
  ]
}